@inproceedings{Chen2020,  
    author={Qinyue Chen and Sheue-Er Low and Jeremiah W.E. Yap and Adjovi K.X. Sim and Yu-Yang Tan and Benjamin W.J. Kwok and Jeannie S.A. Lee and Chek-Tien Tan and Wan-Ping Loh and Bernard L.W. Loo and Adison C.K. Wong},  
    booktitle={2020 IEEE International Conference on Teaching, Assessment, and Learning for Engineering (TALE)},   
    title={Immersive Virtual Reality Training of Bioreactor Operations},   
    year={2020},  
    pages={873-878},  
    doi={10.1109/TALE48869.2020.9368468}
} 
@inproceedings{Zhao2019,
    author = {Zhao, Desheng and Lee, Jeannie Su Ann and Tan, Chek Tien and Dancu, Alexandru and Lui, Simon and Shen, Songjia and Mueller, Florian `Floyd'},
    title = {GameLight - Gamification of the Outdoor Cycling Experience},
    booktitle = {Companion Publication of the 2019 on Designing Interactive Systems Conference 2019 Companion},
    series = {DIS '19 Companion},
    year = {2019},
    isbn = {978-1-4503-6270-2},
    location = {San Diego, CA, USA},
    pages = {73--76},
    numpages = {4},
    doi = {10.1145/3301019.3325151},
    acmid = {3325151},
    publisher = {ACM},
    address = {New York, NY, USA},
    keywords = {bicycle, cycling, exergames, human computer interaction, mobile phone, projector, serious games},
} 
@article{Tan2018b,
    author = {Beena Ahmed and Penelope Monroe and Adam Hair and Chek Tien Tan and Ricardo Gutierrez-Osuna and Kirrie J. Ballard},
    title = {Speech-driven mobile games for speech therapy: User experiences and feasibility},
    journal = {International Journal of Speech-Language Pathology},
    volume = {20},
    number = {6},
    pages = {644-658},
    year  = {2018},
    publisher = {Taylor \& Francis},
    doi = {10.1080/17549507.2018.1513562},
    note ={PMID: 30301384},
    URL = {https://doi.org/10.1080/17549507.2018.1513562},
    eprint = {https://doi.org/10.1080/17549507.2018.1513562}
}
@inproceedings{10.1145/3116595.3116607,
author = {Mueller, Florian 'Floyd' and Tan, Chek Tien and Byrne, Rich and Jones, Matt},
title = {13 Game Lenses for Designing Diverse Interactive Jogging Systems},
year = {2017},
isbn = {9781450348980},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3116595.3116607},
doi = {10.1145/3116595.3116607},
abstract = {HCI is increasingly interested in designing technology for being physically active, and in many cases focuses on jogging. We find that many current approaches seem to view jogging only through a lens of athletic performance. However, jogging is multifaceted, yet there is so far no collated list of alternative lenses through which jogging could be viewed at by designers. In this paper, we draw on game design thinking to articulate 13 lenses through which designers can examine jogging. These 13 lenses are derived from related work and our combined experience of having designed and studied three different jogging systems. The lenses enable a structured articulation of key opportunities that interactive technology offers for jogging designers. With our work, we aim to support designers who want to create diverse interactive jogging systems so that more people can profit from the many benefits of jogging.},
booktitle = {Proceedings of the Annual Symposium on Computer-Human Interaction in Play},
pages = {43–56},
numpages = {14},
keywords = {jogging, exertion, whole-body interaction, sport, running},
location = {Amsterdam, The Netherlands},
series = {CHI PLAY '17}
}
@inproceedings{10.1145/2999508.2999519,
author = {Zhang, Wenlong and Tan, Chek Tien and Chen, Tim},
title = {A Safe Low-Cost HMD for Underwater VR Experiences},
year = {2016},
isbn = {9781450345514},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2999508.2999519},
doi = {10.1145/2999508.2999519},
abstract = {Recently, consumer head-mounted VR displays (HMDs) like the Oculus Rift1 and Samsung Gear VR2 have driven much research into interesting applications that include full-body sensory experiences. For example, PaperDude VR [Bolton et al. 2014] uses the Oculus Rift to implement a cycling exergame in order to motivate exercise. Other than using a consumer HMD like the Rift to provide the VR visuals, PaperDude VR includes a real bicycle on a stationary trainer in order to approximate a realistic experience using other non-visual senses. In another work, Birdly [Rheiner 2014] uses the HTC Vive3 as part of a system to simulate flying. In Birdly, the nonvisual elements are slightly harder to simulate, resulting in a rather complex robotic setup which includes sensory-motor coupling and a large fan to provide wind feedback.},
booktitle = {SIGGRAPH ASIA 2016 Mobile Graphics and Interactive Applications},
articleno = {12},
numpages = {2},
keywords = {HMD, AR, underwater, cybersickness, VR},
location = {Macau},
series = {SA '16}
}
@inproceedings{10.1145/2793107.2793117,
author = {Tan, Chek Tien and Leong, Tuck Wah and Shen, Songjia and Dubravs, Christopher and Si, Chen},
title = {Exploring Gameplay Experiences on the Oculus Rift. **HONORABLE MENTION**},
year = {2015},
isbn = {9781450334662},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2793107.2793117},
doi = {10.1145/2793107.2793117},
abstract = {Compared to previous head-mounted displays, the compact and low-cost Oculus Rift has claimed to offer improved virtual reality experiences. However, how and what kinds of user experiences are encountered by people when using the Rift in actual gameplay has not been examined. We present an exploration of 10 participants' experiences of playing a first-person shooter game using the Rift. Despite cybersickness and a lack of control, participants experienced heightened experiences, a richer engagement with passive game elements, a higher degree of flow and a deeper immersion on the Rift than on a desktop setup. Overly demanding movements, such as the large range of head motion required to navigate the game environment were found to adversely affect gaming experiences. Based on these and other findings, we also present some insights for designing games for the Rift.},
booktitle = {Proceedings of the 2015 Annual Symposium on Computer-Human Interaction in Play},
pages = {253–263},
numpages = {11},
keywords = {psychophysiology, head tracking, think-aloud, head-mounted displays, virtual reality, oculus rift, game user research},
location = {London, United Kingdom},
series = {CHI PLAY '15}
}
@inproceedings{10.1145/2793107.2810263,
author = {Tan, Chek Tien and Mirza-Babaei, Pejman and Zammitto, Veronica and Canossa, Alessandro and Conley, Genevieve and Wallner, Guenter},
title = {Tool Design Jam: Designing Tools for Games User Research},
year = {2015},
isbn = {9781450334662},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2793107.2810263},
doi = {10.1145/2793107.2810263},
abstract = {In both industry and academia, software tools are essential for games user research (GUR) in order to collect, integrate, analyze and report on games and players' data. GUR datasets are becoming more and more complex, detailed and multifaceted. Hence, tools are necessary to efficiently handle data. This one-day workshop explores the vast spectrum of tools used and created by current GUR researchers and provides a platform of discussion for advancing the development of such tools. This workshop will facilitate intersections from user researchers with diverse epistemologies, as well as from both academia and the industry, in an interactive Design Jam activity to collaboratively design future-proof GUR tools. The immediate outcome of the workshop is twofold: to collectively establish state-of-the-art tool design guidelines, and to archive the papers and discussions, which will extend the conversations and relationships beyond the workshop. Moreover, the long-term outcome will be the start of a community that focuses on creating better tools to aid the study of player experiences.},
booktitle = {Proceedings of the 2015 Annual Symposium on Computer-Human Interaction in Play},
pages = {827–831},
numpages = {5},
keywords = {game user research, user studies, mixed methods, research tools, user experience, usability},
location = {London, United Kingdom},
series = {CHI PLAY '15}
}
@inproceedings{10.1145/2818427.2818434,
author = {Tan, Chek Tien and Byrne, Richard and Lui, Simon and Liu, Weilong and Mueller, Florian},
title = {JoggAR: A Mixed-Modality AR Approach for Technology-Augmented Jogging},
year = {2015},
isbn = {9781450339285},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2818427.2818434},
doi = {10.1145/2818427.2818434},
abstract = {JoggAR demonstrates a novel combination of wearable visual, audio and sensing technology to realize a game-like persistent augmented reality (AR) environment to enhance jogging and other exertion experiences that involves changing attention intensities in the course of the activities. In particular we developed a method to perform an audio-first exploration of 3D virtual spaces so as to achieve our experiential goal of supporting exertion-focused activities.},
booktitle = {SIGGRAPH Asia 2015 Mobile Graphics and Interactive Applications},
articleno = {33},
numpages = {1},
location = {Kobe, Japan},
series = {SA '15}
}
@inproceedings{10.1145/2556288.2557326,
author = {Tan, Chek Tien and Leong, Tuck Wah and Shen, Songjia},
title = {Combining Think-Aloud and Physiological Data to Understand Video Game Experiences},
year = {2014},
isbn = {9781450324731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2556288.2557326},
doi = {10.1145/2556288.2557326},
abstract = {Think-aloud protocols are commonly used to evaluate player experiences of video games but suffer from a lack of objectivity and timeliness. On the other hand, quantitative captures of physiological data are effective; providing detailed, unbiased and continuous responses of players, but lack contexts for interpretation. This paper documents how both approaches could be used together in practice by comparing video-cued retrospective think-aloud data and physiological data collected during a video gameplay experiment. We observed that many interesting physiological responses did not feature in participants' think-aloud data, and conversely, reports of interesting experiences were sometimes not observed in the collected physiological data. Through learnings from our experiment, we present some of the challenges when combining these approaches and offer some guidelines as to how qualitative and quantitative data can be used together to gain deeper insights into player experiences.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI '14)},
pages = {381–390},
numpages = {10},
keywords = {think-aloud, game user research, psychophysiology},
location = {Toronto, Ontario, Canada},
series = {CHI '14}
}
@inproceedings{10.5555/3014752.3014758,
author = {Blom, Paris Mavromoustakos and Bakkes, Sander and Tan, Chek Tien and Whiteson, Shimon and Roijers, Diederik and Valenti, Roberto and Gevers, Theo},
title = {Towards Personalised Gaming via Facial Expression Recognition},
year = {2014},
isbn = {1577356810},
publisher = {AAAI Press},
abstract = {In this paper we propose an approach for personalising the space in which a game is played (i.e., levels) dependent on classifications of the user's facial expression - to the end of tailoring the affective game experience to the individual user. Our approach is aimed at online game personalisation, i.e., the game experience is personalised during actual play of the game. A key insight of this paper is that game personalisation techniques can leverage novel computer vision-based techniques to unobtrusively infer player experiences automatically based on facial expression analysis. Specifically, to the end of tailoring the affective game experience to the individual user, in this paper we (1) leverage the proven INSIGHT facial expression recognition SDK as a model of the user's affective state (Sightcorp 2014), and (2) employ this model for guiding the online game personalisation process. User studies that validate the game personalisation approach in the actual video game INFINITE MARIO BROS. reveal that it provides an effective basis for converging to an appropriate affective state for the individual human player.},
booktitle = {Proceedings of the Tenth AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment},
pages = {30–36},
numpages = {7},
location = {Raleigh, NC, USA},
series = {AIIDE'14}
}
@inproceedings{10.1145/2559206.2574785,
author = {Harrold, Natalie and Tan, Chek Tien and Rosser, Daniel and Leong, Tuck Wah},
title = {CopyMe: An Emotional Development Game for Children},
year = {2014},
isbn = {9781450324748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2559206.2574785},
doi = {10.1145/2559206.2574785},
abstract = {Proper emotional development is important for young children, especially those with psychological disorders such as autism spectrum disorders (ASDs), whereby early intervention becomes crucial. However, traditional paper-based interventions are mostly laborious and difficult to employ for carers and parents, whilst current computer-aided interventions fee too much like obvious assistive tools and lack timely feedback to inform and aid progress. CopyMe is an iPad game we developed that allows children to learn emotions with instant feedback on performance. A pilot study revealed children with ASDs were able to enjoy and perform well in the game. CopyMe also demonstrates a novel affective game interface that incorporates state-of-the-art facial expression tracking and classification. This will be particularly interesting for CHI attendees working in the domain of affective interfaces and serious games, especially those that target children.},
booktitle = {CHI '14 Extended Abstracts on Human Factors in Computing Systems},
pages = {503–506},
numpages = {4},
keywords = {facial expression recognition, affective interfaces, children, emotional development, serious games},
location = {Toronto, Ontario, Canada},
series = {CHI EA '14}
}
@inproceedings{10.1145/2559206.2574808,
author = {Garcia, Jaime Andres and Pisan, Yusuf and Tan, Chek Tien and Felix Navarro, Karla},
title = {Step Kinnection: A Hybrid Clinical Test for Fall Risk Assessment in Older Adults},
year = {2014},
isbn = {9781450324748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2559206.2574808},
doi = {10.1145/2559206.2574808},
abstract = {In this paper, we describe Step Kinnection, an interactive step training system for the elderly that incorporates mechanisms to simultaneously perform a hybrid clinical test for fall risk assessment. The interactivity demonstration includes a simple stepping task along with three voice-enabled cognitive activities allowing for the assessment of stepping performance under the dual-task paradigm. CHI attendees can try out both scenarios to physically experience the interference caused by a higher cognitive load while stepping.},
booktitle = {CHI '14 Extended Abstracts on Human Factors in Computing Systems},
pages = {471–474},
numpages = {4},
keywords = {stepping performance, kinect, reaction time test, fall risk assessment, elderly},
location = {Toronto, Ontario, Canada},
series = {CHI EA '14}
}
@incollection{Tan2013b,
    address = {Sydney, Australia},
    author = {Tan, Chek Tien and Ferguson, Sam},
    booktitle = {Interactive Experience in the Digital Age - Evaluating New Art Practice},
    chapter = {10},
    edition = {1},
    editor = {Candy, Linda and Edmonds, Ernest},
    isbn = {978-3-319-04509-2},
    pages = {139--152},
    publisher = {Springer},
    title = {{The Role of Emotions in Art Evaluation}},
    year = {2014}
}
@inproceedings{10.1145/2543651.2543657,
author = {Tan, Chek Tien and Harrold, Natalie and Rosser, Daniel},
title = {Can You CopyMe? An Expression Mimicking Serious Game},
year = {2013},
isbn = {9781450326339},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2543651.2543657},
doi = {10.1145/2543651.2543657},
abstract = {Emotional development is an important aspect in the early years of growth in young children, especially those with psychological disorders like autism spectrum disorders (ASDs), whereby emotional development becomes crucial. However, traditional paper-based interventions for emotional development are mostly laborious and difficult to employ for carers and parents, whilst current computer-aided interventions feel too much like obvious assistive tools and lack timely feedback to inform and aid progress. This paper hence describes the design of CopyMe, a touch-based tablet application that uses game design concepts and state-of-the-art computer vision techniques to provide an engaging experience as well as real-time feedback on the children's progress.},
booktitle = {SIGGRAPH Asia 2013 Symposium on Mobile Graphics and Interactive Applications},
articleno = {73},
numpages = {4},
keywords = {facial expression recognition, emotional development, serious games},
location = {Hong Kong, Hong Kong},
series = {SA '13}
}
@inproceedings{10.1145/2542355.2542388,
author = {Tan, Chek Tien and Rosser, Daniel and Harrold, Natalie},
title = {Crowdsourcing Facial Expressions Using Popular Gameplay},
year = {2013},
isbn = {9781450326292},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2542355.2542388},
doi = {10.1145/2542355.2542388},
abstract = {Facial expression analysis systems often employ machine learning algorithms that depend a lot on the quality of the face database they are trained on. Unfortunately, generating high quality face databases is a major challenge that is rather time consuming. We have developed BeFaced, a tile-matching casual tablet game to enable massive crowdsourcing of facial expressions for the purpose of such machine learning algorithms. Based on the popular tile-matching gameplay mechanic, players are required to make facial expressions shown on matched tiles in order to clear them and advance in the game. Dynamic difficulty adjustment of the recognition accuracy is employed in the game in order to increase engagement and hence increase the quantity of varied facial expressions obtained. Each facial expression is automatically captured, labelled and sent to our online face database. At a more abstract level, BeFaced investigates a novel method of using popular game mechanics to aid the advancement of computer vision algorithms.},
booktitle = {SIGGRAPH Asia 2013 Technical Briefs},
articleno = {26},
numpages = {4},
keywords = {crowdsourcing, facial expression analysis, serious games},
location = {Hong Kong, Hong Kong},
series = {SA '13}
}
@inproceedings{10.5555/3014666.3014701,
author = {Tan, Chek Tien and Cheng, Ho-lun},
title = {An Automated Model-Based Adaptive Architecture in Modern Games},
year = {2010},
publisher = {AAAI Press},
abstract = {This paper proposes an automatic model-based approach that enables adaptive decision making in modern virtual games. It builds upon the Integrated MDP and POMDP Learning AgeNT (IMPLANT) architecture (Tan and Cheng 2009) which has shown to provide plausible adaptive decision making in modern games. However, it suffers from highly time-consuming manual model specification problems. By incorporating an automated priority sweeping based model builder for the MDP, as well as using the Tactical Agent Personality (Tan and Cheng 2007) for the POMDP, the work in this paper aims to resolve these problems. Empirical proof of concept is shown based on an implementation in a modern game scenario, whereby the enhanced IMPLANT agent is shown to exhibit superior adaptation performance over the old IMPLANT agent whilst eliminating manual model specifications and at the same time still maintaining plausible speeds.},
booktitle = {Proceedings of the Sixth AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment},
pages = {186–191},
numpages = {6},
location = {Stanford, California, USA},
series = {AIIDE'10}
}
